---
output:
  github_document:
    pandoc_args: --webtex=https://chart.googleapis.com/chart?cht=tx&chl=
bibliography: README.bib
nocite: |
  @Polyak92
---

# Multivariate State Space Models
[![Build Status on Travis](https://travis-ci.org/boennecd/mssm.svg?branch=master,osx)](https://travis-ci.org/boennecd/mssm)

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  error = FALSE, cache = "./README-cache/", fig.path = "./README-fig/", 
  echo = TRUE)
options(digits = 4, scipen = 7)
.fig_height_small <- 4
source("render_toc.R")
```

This package provides methods to estimate models of the form 

$$y_{it} \sim g(\eta_{it}),\qquad i\in I_t$$ 
$$\eta_{it} = \vec\gamma^\top\vec x_{it} +\vec\beta_t^\top\vec z_{it}$$
$$\vec\beta_t = F\vec\beta_{t-1}+\vec\epsilon_t, \qquad \vec\epsilon_t\sim N(\vec 0, Q)$$

where $g$ is simple distribution, we observe $t=1,\dots,T$ periods, and $I_t$, 
$y_{it}$, $\vec x_{it}$, and 
$\vec z_{it}$ are known. What is multivariate is 
$\vec y_t = \{y_{it}\}_{i\in I_t}$ (though, $\vec \beta_t$ can also be 
multivariate) and this package is written to scale well 
in the dimension of $| I_t |$. The package uses an independent 
particle filter as suggested by @Lin05. This is particular type of filter 
can be used in the method suggested by @Poyiadjis11. I will show an example
of how to use the package through the rest of the document and highlight some 
implementation details. 

The package is not on CRAN but you can be installed from Github using e.g., 

```{r eval = FALSE}
devtools::install_github("boennecd/mssm")
```


## Table of Contents

```{r echo = FALSE}
render_toc("README.Rmd", toc_header_name = "Table of Contents", toc_depth = 3L)
```

## Poisson Example

We simulate data as follows.

```{r simulate, fig.height = .fig_height_small}
# simulate state path of state variable 
set.seed(78727269)
n_periods <- 110L
(F. <- matrix(c(.5, .1, 0, .8), 2L))
(Q <- matrix(c(.5^2, .1, .1, .7^2), 2L))
(Q_0 <- matrix(c(0.333, 0.194, 0.194, 1.46), 2L))

betas <- cbind(crossprod(chol(Q_0),        rnorm(2L)                      ), 
               crossprod(chol(Q  ), matrix(rnorm((n_periods - 1L) * 2), 2)))
betas <- t(betas)
for(i in 2:nrow(betas))
  betas[i, ] <- betas[i, ] + F. %*% betas[i - 1L, ]
par(mar = c(5, 4, 1, 1))
matplot(betas, lty = 1, type = "l")

# simualte observations
cfix <- c(-1, .2, .5, -1) # gamma
n_obs <- 100L
dat <- lapply(1:n_obs, function(id){
  x <- runif(n_periods, -1, 1)
  X <- cbind(X1 = x, X2 = runif(1, -1, 1))
  z <- runif(n_periods, -1, 1)
  
  eta <- drop(cbind(1, X, z) %*% cfix + rowSums(cbind(1, z) * betas))
  y <- rpois(n_periods, lambda = exp(eta))
  
  # randomly drop some
  keep <- .2 > runif(n_periods)
  
  data.frame(y = y, X, Z = z, id = id, time_idx = 1:n_periods)[keep, ]
})
dat <- do.call(rbind, dat)

# show some properties 
nrow(dat)
head(dat)
table(dat$y)
```

In the above, we simulate `r n_periods` (`n_periods`) with `r n_obs` (`n_obs`)
individuals. Each individual has a fixed covaraite, `X2`, and two time-varying
covariates, `X1` and `Z`. One of the time-varying covariates, `Z`, has a 
random slope. Further, the intercept is also random. 

### Log-Likelihood Approximations

We estimate a generalized linear model without random effects.

```{r fit_glm}
glm_fit <- glm(y ~ X1 + X2 + Z, poisson(), dat)
summary(glm_fit)
logLik(glm_fit)
```

Next, we make a log-likelihood approximation with the implemented particle at 
the true parameters with the `mssm` function.

```{r fit_mssm}
library(mssm)
ll_func <- mssm(
  fixed = y ~ X1 + X2 + Z, family = poisson(), data = dat, 
  # make it explict that there is an intercept (not needed)
  random = ~ 1 + Z, ti = time_idx, control = mssm_control(
    n_threads = 5L, N_part = 500L, what = "log_density"))

system.time(
  mssm_obj <- ll_func$pf_filter(
    cfix = cfix, disp = numeric(), F. = F., Q = Q))

# returns the log-likelihood approximation
logLik(mssm_obj)
```

As expected, we get a much higher log-likelihood. We can e.g., compare this 
to a model where we use splines instead for each of the two random effects.

```{r use_splines}
# compare with GLM with spline
library(splines)
logLik(glm(y ~ X1 + X2 + Z * ns(time_idx, df = 20, intercept = TRUE)- 1, 
           poisson(), dat))
```

We can plot the **filter** estimates.

```{r plot_filter, fig.height = .fig_height_small}
# plot estiamtes 
filter_means <- plot(mssm_obj)

# plot with true line as well
for(i in 1:ncol(betas)){
  be <- betas[, i]
  me <- filter_means$means[i, ]
  lb <- filter_means$lbs[i, ]
  ub <- filter_means$ubs[i, ]
  
  # dashed: true line, continuous: filter estimate 
  # dotted: prediction interval
  par(mar = c(5, 4, 1, 1))
  matplot(cbind(be, me, lb, ub), lty = c(2, 1, 3, 3), type = "l", 
          col = "black", ylab = rownames(filter_means$lbs)[i])
}
```

We can get the effective sample size at each point in time with the `get_ess`
function.

```{r show_ess, fig.height = .fig_height_small}
(ess <- get_ess(mssm_obj))
plot(ess)
```

We can compare this what we get by using a so-called bootstrap (like) filter
instead.

```{r comp_boot, fig.height = .fig_height_small}
local({
  ll_boot <- mssm(
    fixed = y ~ X1 + X2 + Z, family = poisson(), data = dat, 
    random = ~ Z, ti = time_idx, control = mssm_control(
      n_threads = 5L, N_part = 500L, what = "log_density", 
      which_sampler = "bootstrap"))
  
  print(system.time(
    boot_fit <- ll_boot$pf_filter(
      cfix = coef(glm_fit), disp = numeric(), F. = F., Q = Q)))
  
  plot(get_ess(boot_fit))
})
```

The above is not much faster (and maybe slower in this run) as the bulk of 
the computation is not in the sampling step. We can also compare the 
log-likelihood approximation with what we get if we choose parameters close 
to the GLM estimates.

```{r comp_close_glm}
mssm_glm <- ll_func$pf_filter(
  cfix = coef(glm_fit), disp = numeric(), F. = diag(1e-8, 2), 
  Q = diag(1e-4^2, 2))
logLik(mssm_glm)
```

TODO: I am not sure why we get a lower log-likelihood with the above. 

### Parameter Estimation

We will need to estimate the parameter for real applications. We could do 
this e.g., with a Monte Carlo expectation-maximization algorithm or by using 
a Monte Carlo approximation of the gradient. Currently, the latter is only 
available and the user will have to write a custom function to perform 
the estimation. 
I will provide an example below. The `sgd` function is not a part of the 
package. Instead the package provides a way to approximate the gradient and 
allows the user to perform subsequent maximization. The definition of the 
`sgd` is given at the end of this file as it is somewhat long. 

```{r mc_grad_est, echo = FALSE}
# Stochastic gradient descent for mssm object. The function assumes that the 
# state vector is stationary. The default values are rather arbitrary.
# 
# Args:
#   object: an object of class mssmFunc. 
#   n_it: number of iterations. 
#   lrs: learning rates to use. Must have n_it elements. Like problem specific. 
#   avg_start: index to start averaging. See Polyak et al. (1992) for arguments
#              for averaging.
#   cfix: starting values for fixed coefficients. 
#   F.: starting value for transition matrix in conditional distribution of the 
#       current state given the previous state.
#   Q: starting value for covariance matrix in conditional distribution of the 
#      current state given the previous. 
#   verbose: TRUE if output should be printed doing estmation. 
# 
# Returns:
#   List with estimates and the log-likelihood approximation at each iteration. 
sgd <- function(
  object, n_it = 250L, 
  lrs = 1e-3 * (1:n_it)^(-1/2), avg_start = max(1L, as.integer(n_it * 4L / 5L)),
  cfix, F., Q,  verbose = FALSE)
{
  # make checks
  stopifnot(
    inherits(object, "mssmFunc"), object$control$what == "gradient", n_it > 0L, 
    all(lrs > 0.), avg_start > 1L, length(lrs) == n_it)
  n_fix <- nrow(object$X)
  n_rng <- nrow(object$Z)
  
  # objects for estimates at each iteration and log-likelihood approximations
  ests <- matrix(
    NA_real_, n_it + 1L, n_fix + n_rng * n_rng + n_rng * (n_rng  + 1L) / 2L)
  ests[1L, ] <- c(cfix, F., Q[lower.tri(Q, diag = TRUE)])
  lls <- rep(NA_real_, n_it)
  
  # indices of the different components
  idx_fix <- 1:n_fix
  idx_F   <- 1:(n_rng * n_rng) + n_fix
  idx_Q   <- 1:(n_rng * (n_rng  + 1L) / 2L) + n_fix + n_rng * n_rng
  
  # we only want the lower part of `Q` so we make the following map for the 
  # gradient
  library(matrixcalc) # TODO: get rid of this
  gr_map <- matrix(
    0., nrow = ncol(ests), ncol = length(cfix) + length(F.) + length(Q))
  gr_map[idx_fix, idx_fix] <- diag(length(idx_fix))
  gr_map[idx_F  , idx_F] <- diag(length(idx_F))
  dup_mat <- duplication.matrix(ncol(Q))
  gr_map[idx_Q  , -c(idx_fix, idx_F)] <- t(dup_mat)
  
  # function to set the parameters
  set_parems <- function(i){
    # select whether or not to average
    idx <- if(i > avg_start) avg_start:i else i
    
    # set new parameters
    cfix <<-             colMeans(ests[idx, idx_fix, drop = FALSE])
    F.[] <<-             colMeans(ests[idx, idx_F  , drop = FALSE])
    Q[]  <<- dup_mat %*% colMeans(ests[idx, idx_Q  , drop = FALSE])
    
  }
    
  # run gradient decent 
  max_half <- 25L
  for(i in 1:n_it + 1L){
    # get gradient. First, run the particle filter
    filter_out <- object$pf_filter(
      cfix = cfix, disp = numeric(), F. = F., Q = Q)
    lls[i - 1L] <- c(logLik(filter_out))
    
    # then get the gradient associated with each particle and the log 
    # normalized weight of the particles
    grads <- tail(filter_out$pf_output, 1L)[[1L]]$stats
    ws    <- tail(filter_out$pf_output, 1L)[[1L]]$ws_normalized
    
    # compute the gradient and take a small step
    grad <- colSums(t(grads) * drop(exp(ws)))
    lr_i <- lrs[i - 1L]
    k <- 0L
    while(k < max_half){
      ests[i, ] <- ests[i - 1L, ] + lr_i * gr_map %*% grad 
      set_parems(i)
      
      # check that Q is positive definite and the system is stationary
      c1 <- all(abs(eigen(F.)$values) < 1)
      c2 <- all(eigen(Q)$values > 0)
      if(c1 && c2)
        break
      
      # decrease learning rate
      lr_i <- lr_i * .5
      k <- k + 1L
    }
    
    # check if we failed to find a value within our constraints
    if(k == max_half)
      stop("failed to find solution within constraints")
    
    # print information if requested 
    if(verbose){
      cat(sprintf(
        "\nIt %5d: log-likelihood (current, max) %12.2f, %12.2f\n", 
        i - 1L, logLik(filter_out), max(lls, na.rm = TRUE)), 
          rep("-", 66), "\n", sep = "")
      cat("cfix\n")
      print(cfix)
      cat("F\n")
      print(F.)
      cat("Q\n")
      print(Q)
      
    }
  } 
  
  list(estimates = ests, logLik = lls, F. = F., Q = Q, cfix = cfix)
}
```

```{r use_sgd, cache = 1}
# setup mssmFunc object to use
ll_func <- mssm(
  fixed = y ~ X1 + X2 + Z, family = poisson(), data = dat, 
  random = ~ Z, ti = time_idx, control = mssm_control(
    n_threads = 5L, N_part = 200L, what = "gradient"))

# use stochastic gradient descent
system.time(
  res <- sgd(
    ll_func, F. = diag(.1, 2), Q = diag(.1^2, 2), cfix = coef(glm_fit)))
```

A plot of the approximate log-likelihoods at each iteration are shown below 
along with the final estimates. 

```{r show_use_sgd, fig.height = .fig_height_small}
tail(res$logLik, 1L) # final log-likelihood approximation
par(mar = c(5, 4, 1, 1))
plot(     res$logLik       , type = "l")
plot(tail(res$logLik, 100L), type = "l") # only the final iterations

# final estimates
res$F. 
res$Q
res$cfix
```

### Faster Approximation
One drawback with the particle filter we use is that it has $\mathcal{O}(N^2)$ 
computational complexity where $N$ is the number of particles. We can see 
this by adjusting the number of particles. 

```{r comp_w_n_part, cache = 1}
local({
  # assign function that returns a function that use given number of particles
  func <- function(N){
    ll_func <- mssm(
      fixed = y ~ X1 + X2 + Z, family = poisson(), data = dat, 
      random = ~ Z, ti = time_idx, control = mssm_control(
        n_threads = 5L, N_part = N, what = "log_density"))
    function()
      ll_func$pf_filter(
        cfix = coef(glm_fit), disp = numeric(), F. = diag(1e-8, 2), 
        Q = diag(1e-4^2, 2))
      
  }
  
  f_100  <- func( 100)
  f_200  <- func( 200)
  f_400  <- func( 400)
  f_800  <- func( 800)
  f_1600 <- func(1600)
  
  # benchmark. Should grow ~ N^2
  microbenchmark::microbenchmark(
    `100` = f_100(), `200` = f_200(), `400` = f_400(), `800` = f_800(),
    `1600` = f_1600(), times = 3L)
})
```

A solution is to use the dual k-d tree method I cover later. The computational
complexity is $\mathcal{O}(N \log N)$ for this method which is somewhat 
indicated by the run times shown below.

```{r KD_comp_w_n_part, cache = 1}
local({
  # assign function that returns a function that use given number of particles
  func <- function(N){
    ll_func <- mssm(
      fixed = y ~ X1 + X2 + Z, family = poisson(), data = dat, 
      random = ~ Z, ti = time_idx, control = mssm_control(
        n_threads = 5L, N_part = N, what = "log_density", 
        which_ll_cp = "KD", KD_N_max = 6L, aprx_eps = 1e-2))
    function()
      ll_func$pf_filter(
        cfix = coef(glm_fit), disp = numeric(), F. = diag(1e-8, 2), 
        Q = diag(1e-4^2, 2))
      
  }
  
  f_100   <- func(  100)
  f_200   <- func(  200)
  f_400   <- func(  400)
  f_800   <- func(  800)
  f_1600  <- func( 1600)
  f_51200 <- func(51200)
  
  # benchmark. Should grow ~ N log N
  microbenchmark::microbenchmark(
    `100` = f_100(), `200` = f_200(), `400` = f_400(), `800` = f_800(), 
    `1600` = f_1600(), `51200` = f_51200(), times = 3L)
})
```

The `aprx_eps` controls the size of the error. To be precise about what this
value does then we need to some notation for the complete likelihood 
(the likelihood where we observe $\vec\beta_1,\dots,\vec\beta_T$s). This is

$$L = \mu_1(\vec \beta_1)g_1(\vec y_1 \mid \vec \beta_1)\prod_{t=2}^Tf(\vec\beta_t \mid\vec\beta_{t-1})g_t(y_t\mid\beta_t)$$

where $g_t$ is conditional distribution $\vec y_t$ given $\vec\beta_t$, $f$ is 
the conditional distribution of $\vec\beta_t$ given $\vec\beta_{t-1}$, and
$\mu$ is the time-invariant distribution of $\vec\beta$. Let $w_t^{(j)}$ be the weight of particle 
$j$ at time $t$ and $\vec \beta_t^{(j)}$ be the $j$th particle at time $t$. 
Then we ensure the error in our evaluation of terms 
$w_{t-1}^{(j)}f(\vec\beta_t^{(i)} \mid \vec\beta_{t-1}^{(j)})$ never 
exceeds

$$w_{t-1} \frac{u - l}{(u + l)/2}$$
where $u$ and $l$ are respectively an upper and lower bound of 
$f(\vec\beta_t^{(i)} \mid \vec\beta_{t-1}^{(j)})$. 
The question is how big the error is. 
Thus, we consider the error in the log-likelihood approximation at the 
true parameters. 

```{r comp_arell_aprx, cache = 1, fig.height = .fig_height_small}
ll_compare <- local({
  N_use <- 500L
  # we alter the seed in each run
  ll_no_approx <- sapply(1:100, function(seed){
    ll_func <- mssm(
      fixed = y ~ X1 + X2 + Z, family = poisson(), data = dat,
      random = ~ Z, ti = time_idx, control = mssm_control(
        n_threads = 5L, N_part = N_use, what = "log_density", 
        seed = seed))
    
    logLik(ll_func$pf_filter(
      cfix = cfix, disp = numeric(), F. = F., Q = Q))
  })
  
  ll_approx <- sapply(1:100, function(seed){
    ll_func <- mssm(
      fixed = y ~ X1 + X2 + Z, family = poisson(), data = dat,
      random = ~ Z, ti = time_idx, control = mssm_control(
        n_threads = 5L, N_part = N_use, what = "log_density", 
        KD_N_max = 6L, aprx_eps = 1e-2, seed = seed, 
        which_ll_cp = "KD"))
    
    logLik(ll_func$pf_filter(
      cfix = cfix, disp = numeric(), F. = F., Q = Q))
  })
  
  list(ll_no_approx = ll_no_approx, ll_approx = ll_approx)
}) 
```

```{r show_comp_arell_aprx, fig.height = .fig_height_small}
par(mar = c(5, 4, 1, 1))
hist(
  ll_compare$ll_no_approx, main = "", breaks = 20L, 
  xlab = "Log-likelihood approximation -- no aprox")
hist(
  ll_compare$ll_approx   , main = "", breaks = 20L, 
  xlab = "Log-likelihood approximation -- aprox")
```

The latter seems to have a small positive bias.

```{r}
with(ll_compare, t.test(ll_no_approx, ll_approx))
```

The fact that it is small is nice because now we can get a much better 
approximation quickly of e.g., the log-likelihood as shown below.

```{r show_ll_quick, cache = 1}
ll_approx <- sapply(1:10, function(seed){
  N_use <- 100000L
  
  ll_func <- mssm(
    fixed = y ~ X1 + X2 + Z, family = poisson(), data = dat,
    random = ~ Z, ti = time_idx, control = mssm_control(
      n_threads = 5L, N_part = N_use, what = "log_density", 
      KD_N_max = 100L, aprx_eps = 1e-2, seed = seed, 
      which_ll_cp = "KD"))
  
  logLik(ll_func$pf_filter(
    cfix = cfix, disp = numeric(), F. = F., Q = Q))
}) 

# approximate log-likelihood
sd(ll_approx)

# compare sd with 
sd(ll_compare$ll_no_approx)
```

## Fast Sum-Kernel Approximation

This package contains a simple implementation of the dual-tree method like the one 
suggested by @Gray03 and shown in @Klaas06. It is currently not used in 
the particle filters but will be in the future. The problem we want to solve is
the sum-kernel problem in @Klaas06. Particularly, we consider the situation 
where we have $1,\dots,N_q$ query particles denoted by 
$\{\vec Y_i\}_{i=1,\dots,N_q}$ and $1,\dots,N_s$ source particles denoted by
$\{\vec X_j\}_{j=1,\dots,N_s}$. For each query particle, we want to compute 
the weights 

$$W_i = \frac{\tilde W_i}{\sum_{k = 1}^{N_q} \tilde W_i},\qquad \tilde W_i = \sum_{j=1}^{N_s} \bar W_j K(\vec Y_i, \vec X_j)$$

where each source particle has an associated weight $\bar W_j$ and $K$ is a
kernel. Computing the above is $\mathcal{O}(N_sN_q)$ which is major 
bottleneck if $N_s$ and $N_q$ is large. However, one can use a 
[k-d tree](https://en.wikipedia.org/wiki/K-d_tree) for the query particles
and source particles and exploit that:

- $W_j K(\vec Y_i, \vec X_j)$ is almost zero for some pairs of nodes in the 
two k-d trees. 
- $K(\cdot, \vec X_j)$ is almost identical for some nodes in the k-d tree 
for the source particles. 

Thus, a substantial amount of computation can be skipped or approximated 
with e.g., the centroid in the source node with only a minor
loss of precision. The dual-tree approximation method is 
$\mathcal{O}(N_s\log N_s)$ and $\mathcal{O}(N_q\log N_q)$.
We start by defining a function to simulate the source 
and query particles (we will let the two sets be identical for simplicity). 
Further, we plot one draw of simulated points and illustrate the leafs in 
the k-d tree.

```{r sim_func}
######
# define function to simulate data
mus <- matrix(c(-1, -1, 
                 1, 1, 
                -1, 1), 3L, byrow = FALSE)
mus <- mus * .75
Sig <- diag(c(.5^2, .25^2))

get_sims <- function(n_per_grp){
  # simulate X
  sims <- lapply(1:nrow(mus), function(i){
    mu <- mus[i, ]
    X <- matrix(rnorm(n_per_grp * 2L), nrow = 2L)
    X <- t(crossprod(chol(Sig), X) + mu)
    
    data.frame(X, grp = i)
  })
  sims <- do.call(rbind, sims)
  X <- t(as.matrix(sims[, c("X1", "X2")]))
  
  # simulate weights
  ws <- exp(rnorm(ncol(X)))
  ws <- ws / sum(ws)
  
  list(sims = sims, X = X, ws = ws)
}

#####
# show example 
set.seed(42452654)
invisible(list2env(get_sims(5000L), environment()))

# plot points
par(mar = c(5, 4, .5, .5))
plot(as.matrix(sims[, c("X1", "X2")]), col = sims$grp + 1L)

# find k-d tree and add borders 
out <- mssm:::test_KD_note(X, 50L)
out$indices <- out$indices + 1L
n_ele <- drop(out$n_elems)
idx <- mapply(`:`, cumsum(c(1L, head(n_ele, -1))), cumsum(n_ele))
stopifnot(all(sapply(idx, length) == n_ele))
idx <- lapply(idx, function(i) out$indices[i])
stopifnot(!anyDuplicated(unlist(idx)), length(unlist(idx)) == ncol(X))

grps <- lapply(idx, function(i) X[, i])

borders <- lapply(grps, function(x) apply(x, 1, range))
invisible(lapply(borders, function(b) 
  rect(b[1, "X1"], b[1, "X2"], b[2, "X1"], b[2, "X2"])))
```

Next, we compute the run-times for the previous examples and compare the 
approximations of the un-normalized log weights, $\log \tilde W_i$, and 
normalized weights, $W_i$. The `n_threads` sets the number of threads to 
use in the methods.

```{r comp_run_times, cache = 1}
# run-times
microbenchmark::microbenchmark(
  `dual tree 1` = mssm:::FSKA (X = X, ws = ws, Y = X, N_min = 10L, 
                               eps = 5e-3, n_threads = 1L),
  `dual tree 6` = mssm:::FSKA (X = X, ws = ws, Y = X, N_min = 10L, 
                               eps = 5e-3, n_threads = 4L),
  `naive 1`     = mssm:::naive(X = X, ws = ws, Y = X, n_threads = 1L),
  `naive 6`     = mssm:::naive(X = X, ws = ws, Y = X, n_threads = 4L),
  times = 10L)

# The functions return the un-normalized log weights. We first compare
# the result on this scale
o1 <- mssm:::FSKA  (X = X, ws = ws, Y = X, N_min = 10L, eps = 5e-3, 
                    n_threads = 1L)
o2 <- mssm:::naive(X = X, ws = ws, Y = X, n_threads = 4L)

all.equal(o1, o2)
par(mar = c(5, 4, .5, .5))
hist((o1 - o2)/ abs((o1 + o2) / 2), breaks = 50, main = "", 
     xlab = "Delta un-normalized log weights")

# then we compare the normalized weights
func <- function(x){
  x_max <- max(x)
  x <- exp(x - x_max)
  x / sum(x)
}

o1 <- func(o1)
o2 <- func(o2)
all.equal(o1, o2)
hist((o1 - o2)/ abs((o1 + o2) / 2), breaks = 50, main = "", 
     xlab = "Delta normalized log weights")
```

Finally, we compare the run-times as function of $N = N_s = N_q$. The dashed 
line is "naive" method, the continuous line is the dual-tree method, and the 
dotted line is dual-tree method using 1 thread.

```{r run_times_N, cache = 1}
Ns <- 2^(7:14)
run_times <- lapply(Ns, function(N){
  invisible(list2env(get_sims(N), environment()))
  microbenchmark::microbenchmark(
    `dual-tree`   = mssm:::FSKA (X = X, ws = ws, Y = X, N_min = 10L, eps = 5e-3, 
                                 n_threads = 4L),
    naive         = mssm:::naive(X = X, ws = ws, Y = X, n_threads = 4L),
    `dual-tree 1` = mssm:::FSKA (X = X, ws = ws, Y = X, N_min = 10L, eps = 5e-3, 
                                 n_threads = 1L), 
    times = 5L)
})

Ns_xtra <- 2^(15:19)
run_times_xtra <- lapply(Ns_xtra, function(N){
  invisible(list2env(get_sims(N), environment()))
  microbenchmark::microbenchmark(
    `dual-tree` = mssm:::FSKA (X = X, ws = ws, Y = X, N_min = 10L, eps = 5e-3, 
                               n_threads = 4L),
    times = 5L)
})
```
 
```{r plot_run_times_N}
library(microbenchmark)
meds <- t(sapply(run_times, function(x) summary(x, unit = "s")[, "median"]))
meds_xtra <- 
  sapply(run_times_xtra, function(x) summary(x, unit = "s")[, "median"])
meds <- rbind(meds, cbind(meds_xtra, NA_real_, NA_real_))
dimnames(meds) <- list(
  N = c(Ns, Ns_xtra) * 3L, method = c("Dual-tree", "Naive", "Dual-tree 1"))
meds
par(mar = c(5, 4, .5, .5))
matplot(c(Ns, Ns_xtra) * 3L, meds, lty = 1:3, type = "l", log = "xy", 
        ylab = "seconds", xlab = "N", col = "black")
```

## Function Definitions

```{r mc_grad_est, eval = FALSE}
```

# References
